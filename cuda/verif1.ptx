//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31833905
// Cuda compilation tools, release 11.8, V11.8.89
// Based on NVVM 7.0.1
//

.version 7.8
.target sm_80
.address_size 64

	// .globl	vec_add_i16x4

.visible .entry vec_add_i16x4(
	.param .u64 vec_add_i16x4_param_0,
	.param .u64 vec_add_i16x4_param_1,
	.param .u64 vec_add_i16x4_param_2,
	.param .u32 vec_add_i16x4_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<21>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [vec_add_i16x4_param_0];
	ld.param.u64 	%rd2, [vec_add_i16x4_param_1];
	ld.param.u64 	%rd3, [vec_add_i16x4_param_2];
	ld.param.u32 	%r2, [vec_add_i16x4_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd8];
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	add.s16 	%rs17, %rs12, %rs4;
	add.s16 	%rs18, %rs11, %rs3;
	add.s16 	%rs19, %rs10, %rs2;
	add.s16 	%rs20, %rs9, %rs1;
	st.global.v4.u16 	[%rd10], {%rs20, %rs19, %rs18, %rs17};

$L__BB0_2:
	ret;

}
	// .globl	vec_sub_i16x4
.visible .entry vec_sub_i16x4(
	.param .u64 vec_sub_i16x4_param_0,
	.param .u64 vec_sub_i16x4_param_1,
	.param .u64 vec_sub_i16x4_param_2,
	.param .u32 vec_sub_i16x4_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<21>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [vec_sub_i16x4_param_0];
	ld.param.u64 	%rd2, [vec_sub_i16x4_param_1];
	ld.param.u64 	%rd3, [vec_sub_i16x4_param_2];
	ld.param.u32 	%r2, [vec_sub_i16x4_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd8];
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	sub.s16 	%rs17, %rs4, %rs12;
	sub.s16 	%rs18, %rs3, %rs11;
	sub.s16 	%rs19, %rs2, %rs10;
	sub.s16 	%rs20, %rs1, %rs9;
	st.global.v4.u16 	[%rd10], {%rs20, %rs19, %rs18, %rs17};

$L__BB1_2:
	ret;

}
	// .globl	relu_i16x4
.visible .entry relu_i16x4(
	.param .u64 relu_i16x4_param_0,
	.param .u64 relu_i16x4_param_1,
	.param .u32 relu_i16x4_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [relu_i16x4_param_0];
	ld.param.u64 	%rd2, [relu_i16x4_param_1];
	ld.param.u32 	%r2, [relu_i16x4_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	max.s16 	%rs9, %rs4, 0;
	max.s16 	%rs10, %rs3, 0;
	max.s16 	%rs11, %rs2, 0;
	max.s16 	%rs12, %rs1, 0;
	st.global.v4.u16 	[%rd7], {%rs12, %rs11, %rs10, %rs9};

$L__BB2_2:
	ret;

}
	// .globl	vec_mul_bf16x4
.visible .entry vec_mul_bf16x4(
	.param .u64 vec_mul_bf16x4_param_0,
	.param .u64 vec_mul_bf16x4_param_1,
	.param .u64 vec_mul_bf16x4_param_2,
	.param .u32 vec_mul_bf16x4_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [vec_mul_bf16x4_param_0];
	ld.param.u64 	%rd2, [vec_mul_bf16x4_param_1];
	ld.param.u64 	%rd3, [vec_mul_bf16x4_param_2];
	ld.param.u32 	%r2, [vec_mul_bf16x4_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [%rd8];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs13};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs17};}

	// end inline asm
	mul.f32 	%f3, %f1, %f2;
	// begin inline asm
	{ mov.b32 %f4, {0,%rs14};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f5, {0,%rs18};}

	// end inline asm
	mul.f32 	%f6, %f4, %f5;
	// begin inline asm
	{ mov.b32 %f7, {0,%rs15};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f8, {0,%rs19};}

	// end inline asm
	mul.f32 	%f9, %f7, %f8;
	// begin inline asm
	{ mov.b32 %f10, {0,%rs16};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f11, {0,%rs20};}

	// end inline asm
	mul.f32 	%f12, %f10, %f11;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f12;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	st.global.v4.u16 	[%rd10], {%rs3, %rs6, %rs9, %rs12};

$L__BB3_2:
	ret;

}
	// .globl	fma_bf16x4
.visible .entry fma_bf16x4(
	.param .u64 fma_bf16x4_param_0,
	.param .u64 fma_bf16x4_param_1,
	.param .u64 fma_bf16x4_param_2,
	.param .u64 fma_bf16x4_param_3,
	.param .u32 fma_bf16x4_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [fma_bf16x4_param_0];
	ld.param.u64 	%rd2, [fma_bf16x4_param_1];
	ld.param.u64 	%rd3, [fma_bf16x4_param_2];
	ld.param.u64 	%rd4, [fma_bf16x4_param_3];
	ld.param.u32 	%r2, [fma_bf16x4_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [%rd7];
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.v4.u16 	{%rs21, %rs22, %rs23, %rs24}, [%rd9];
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.v4.u16 	{%rs25, %rs26, %rs27, %rs28}, [%rd11];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs17};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs21};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f3, {0,%rs25};}

	// end inline asm
	fma.rn.f32 	%f4, %f1, %f2, %f3;
	// begin inline asm
	{ mov.b32 %f5, {0,%rs18};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f6, {0,%rs22};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f7, {0,%rs26};}

	// end inline asm
	fma.rn.f32 	%f8, %f5, %f6, %f7;
	// begin inline asm
	{ mov.b32 %f9, {0,%rs19};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f10, {0,%rs23};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f11, {0,%rs27};}

	// end inline asm
	fma.rn.f32 	%f12, %f9, %f10, %f11;
	// begin inline asm
	{ mov.b32 %f13, {0,%rs20};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f14, {0,%rs24};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f15, {0,%rs28};}

	// end inline asm
	fma.rn.f32 	%f16, %f13, %f14, %f15;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs16, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f12;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs8, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs4, %f4;}

	// end inline asm
	st.global.v4.u16 	[%rd13], {%rs4, %rs8, %rs12, %rs16};

$L__BB4_2:
	ret;

}

